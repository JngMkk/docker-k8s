# 3장 컨테이너를 다루는 표준 아키텍처, 쿠버네티스

## 3. 쿠버네티스 연결을 담당하는 서비스

```
쿠버네티스에서는 외부에서 쿠버네티스 클러스터에 접속하는 방법을 서비스라고 함.
```

### 1) 가장 간단하게 연결하는 노드포트

```
외부에서 쿠버네티스 클러스터의 내부에 접속하는 가장 쉬운 방법은 노드포트 서비스를 이용하는 것.
노드포트 서비스를 설정하면 모든 워커 노드의 특정 포트(노드포트)를 열고 여기로 오는 모든 요청을 노드포트 서비스로 전달함.
그리고 노드포트 서비스는 해당 업무를 처리할 수 있는 파드로 요청을 전달함.
```

![123](https://user-images.githubusercontent.com/87686562/153739123-adbe5488-dd8d-413b-a9c8-50a18ba3de9a.jpg)

#### 노드포트 서비스로 외부에서 접속하기

1. 디플로이먼트로 파드 생성. 이때 이미지는 sysnet4admin 계정에 있는 echo-hname을 사용

   ```
   [root@m-k8s ~]# kubectl create deployment np-pods --image=sysnet4admin/echo-hname
   
   deployment.apps/np-pods created
   ```

2. 파드 확인

   ```
   [root@m-k8s ~]# kubectl get pods
   
   NAME                       READY   STATUS    RESTARTS   AGE
   np-pods-5767d54d4b-vvvrm   1/1     Running   0          9s
   ```

3. 노드포트 서비스 생성

   ```
   [root@m-k8s ~]# kubectl create -f ~/_Book_k8sInfra/ch3/3.3.1/nodeport.yaml
   
   service/np-svc created
   ```

   - 오브젝트 스펙 (nodeport.yaml)

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: np-svc
   spec:
     selector:
       app: np-pods
     ports:
       - name: http
         protocol: TCP
         port: 80
         targetPort: 80
         nodePort: 30000
     type: NodePort
   ```

   ![149](https://user-images.githubusercontent.com/87686562/153739433-a2984305-9700-4a13-b2ef-5b248c8786f8.jpg)

   ```
   기존 파드 구조에서 kind가 Service로 바뀌었고, spec에 컨테이너에 대한 정보가 없음.
   그리고 접속에 필요한 네트워크 관련 정보(protocol, port, targetPort, nodePort)와 서비스의 type을 NodePort로 지정
   ```

4. 노드포트 서비스로 생성한 np-svc 서비스를 확인

   ```
   [root@m-k8s ~]# kubectl get services
   
   NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
   kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        6d10h
   np-svc       NodePort    10.109.36.61   <none>        80:30000/TCP   3m46s
   ```

   ```
   노드포트의 포트 번호가 30000번으로 지정됨. CLUSTER-IP는 쿠버네티스 클러스터의 내부에서 사용하는 IP로 자동으로 지정됨.
   ```

5. 쿠버네티스 클러스터의 워커 노드 IP를 확인

   ```
   [root@m-k8s ~]# kubectl get nodes -o wide
   
   NAME     STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
   m-k8s    Ready    master   6d10h   v1.18.4   192.168.1.10    <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
   w1-k8s   Ready    <none>   6d10h   v1.18.4   192.168.1.101   <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
   w2-k8s   Ready    <none>   6d10h   v1.18.4   192.168.1.102   <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
   w3-k8s   Ready    <none>   6d10h   v1.18.4   192.168.1.103   <none>        CentOS Linux 7 (Core)   3.10.0-1127.19.1.el7.x86_64   docker://1.13.1
   ```

6. 호스트에서 웹 브라우저를 띄우고 192.168.1.101 ~ 103와 30000번으로 접속해 외부에서 접속되는지 확인.

   ![image](https://user-images.githubusercontent.com/87686562/153739557-f437ea69-499f-4fba-bafa-f5642eb18f9c.png)

   ![image](https://user-images.githubusercontent.com/87686562/153739575-31d34bd6-a78f-4235-88da-77d9ed2ad42a.png)

   ![image](https://user-images.githubusercontent.com/87686562/153739589-6c5b980f-855d-432a-93ca-dc1a82b7d262.png)

   ```
   파드가 하나이므로 화면에 보이는 이름은 모두 동일
   ```

#### 부하 분산 테스트하기

```
디플로이먼트로 생성된 파드 1개에 접속하고 있는 중에 파드가 3개로 증가하면 접속이 어떻게 바뀔까?
즉, 부하가 분산되는지(로드밸런서 기능) 확인해 보자.
```

1. 반복적으로 192.168.1.101:30000에 접속해 접속한 파드 이름을 화면에 표시 (powershell)

   ```powershell
   $i=0; while($true)
   {
    % { $i++; write-host -NoNewline "$i $_" }
    (Invoke-RestMethod "http://192.168.1.101:30000")-replace '\n', ' '
   }
   
   1 np-pods-5767d54d4b-vvvrm
   2 np-pods-5767d54d4b-vvvrm
   3 np-pods-5767d54d4b-vvvrm
   4 np-pods-5767d54d4b-vvvrm
   ```

2. 파워쉘로 코드를 실행하고 나면 쿠버네티스 마스터 노드에서 sacle을 실행해 파드를 3개로 증가

   ```
   [root@m-k8s ~]# kubectl scale deployment np-pods --replicas=3
   
   deployment.apps/np-pods scaled
   ```

3. 배포된 파드 확인

   ```
   [root@m-k8s ~]# kubectl get pods
   
   NAME                       READY   STATUS    RESTARTS   AGE
   np-pods-5767d54d4b-46lk5   1/1     Running   0          8s
   np-pods-5767d54d4b-vvvrm   1/1     Running   0          4d2h
   np-pods-5767d54d4b-z55mw   1/1     Running   0          8s
   ```

4. 파워쉘 명령 창을 확인해 표시하는 파드 이름에 배포된 파드 3개가 돌아가면서 표시되는지 확인(부하 분산 확인)

   ```
   56382 np-pods-5767d54d4b-46lk5 
   56383 np-pods-5767d54d4b-vvvrm 
   56384 np-pods-5767d54d4b-z55mw 
   ```

   ```
   어떻게 추가된 파드를 외부에서 추적해 접속하는 것일까?
   이는 노드포트의 오브젝트 스펙에 적힌 np-pods와 디플로이먼트의 이름을 확인해 동일하면 같은 파드라고 간주하기 때문.
   ```

   ```yaml
   spec:
     selector:
       app: np-pods
   ```

#### expose로 노드포트 서비스 생성하기

```
노드포트 서비스는 expose 명령어로도 생상할 수 있음.
```

1. expose 명령어로 서비스로 내보낼 디플로이먼트를 np-pods로 지정

   ```
   [root@m-k8s ~]# kubectl expose deployment np-pods --type=NodePort --name=np-svc-v2 --port=80
   
   service/np-svc-v2 exposed
   ```

   ```
   해당 서비스의 이름은 np-svc-v2로, 타입은 NodePort로 지정(이때 서비스 타입은 반드시 대소문자 구분)
   마지막으로 서비스가 파드로 보내줄 연결 포트를 80번으로 지정
   ```

2. get services 명령어로 생성된 서비스 확인

   ```
   [root@m-k8s ~]# kubectl get services
   
   NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
   kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        6d11h
   np-svc       NodePort    10.109.36.61    <none>        80:30000/TCP   49m
   np-svc-v2    NodePort    10.99.239.216   <none>        80:31828/TCP   95s
   ```

   ```
   expose를 사용하면 노드포트의 포트 번호를 지정할 수 없음.
   포트 번호는 30000~32767에서 임의로 지정됨
   ```

3. 호스트에서 웹 브라우저로 접속

   ![image](https://user-images.githubusercontent.com/87686562/153740694-0a5bc073-a6d1-45c7-951b-825c2301f222.png)

4. 디플로이먼트와 서비스 삭제

   ```
   [root@m-k8s ~]# kubectl delete deployment np-pods
   
   deployment.apps "np-pods" deleted
   
   [root@m-k8s ~]# kubectl delete services np-svc
   
   service "np-svc" deleted
   
   [root@m-k8s ~]# kubectl delete services np-svc-v2
   
   service "np-svc-v2" deleted
   ```

### 2) 사용 목적별로 연결하는 인그레스

```
노드포트 서비스는 포트를 중복 사용할 수 없어서 1개의 노드포트에 1개의 디플로이먼트만 적용됨.
여러 개의 디플로이먼트가 있을 때 인그레스 사용. 이 책에서는 NGINX 인그레스 컨트롤러 사용.

1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속.
   이때 노드포트 서비스를 NGINX 인그레스 컨트롤러로 구성.
2. NGINX 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공.
3. 클러스터 IP 서비스는 사용자를 해당 파드로 연결.
```

1. 테스트용 디플로이먼트 2개 배포

   ```
   kubectl create deployment in-hname-pod --image=sysnet4admin/echo-hname
   kubectl create deployment in-ip-pod --image=sysnet4admin/echo-ip
   ```

2. 파드 상태 확인

   ```
   kubectl get po
   ```

3. NGINX 인그레스 컨트롤러 설치

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.2/ingress-nginx.yaml
   ```

4. NGINX 인그레스 컨트롤러의 파드가 배포됐는지 확인

   NGINX 인그레스 컨트롤러는 default 네임스페이스가 아닌 ingress-nginx 네임스페이스에 속하므로

   -n ingress-nginx 옵션을 추가해야 함.

   ```
   kubectl get po -n ingress-nginx
   ```

5. 인그레스를 사용자 요구 사항에 맞게 설정하려면 경로와 작동을 정의해야 함

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.2/ingress-config.yaml
   ```

   인그레스를 위한 설정 파일은 들어오는 주소 값과 포트에 따라 노출된 서비스를 연결하는 역할을 설정.

   외부에서 주소 값과 노드포트를 가지고 들어오는 것은 hname-svc-default 서비스와 연결된 파드로 넘기고,

   외부에서 들어오는 주소 값, 노드포트와 함께 뒤에 /ip를 추가한 주소 값은 ip-svc 서비스와 연결된 파드로 접속하게 설정

6. 인그레스 설정 파일이 제대로 등록됐는지 확인

   ```
   kubectl get ingress
   ```

7. 인그레스에 요청한 내용이 확실하게 적용됐는지 확인.

   ```
   kubectl get ingress -o yaml
   ```

8. 외부에서 NGINX 인그레스 컨트롤러에 접속할 수 있게 노드포트 서비스로 인끄례쓰 컨트롤러를 외부에 노출

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.2/ingress.yaml
   ```

   - ingress.yaml 내용

     ```yaml
     apiVersion: v1
     kind: Service
     metadata:
       name: nginx-ingress-controller
       namespace: ingress-nginx
     spec:
       ports:
       - name: http
         protocol: TCP
         port: 80
         targetPort: 80
         nodePort: 30100
       - name: https
         protocol: TCP
         port: 443
         targetPort: 443
         nodePort: 30101
       selector:
         app.kubernetes.io/name: ingress-nginx
       type: NodePort
     ```
     기존 노드포트와 달리 http를 처리하기 위해 30100번 포트로 들어온 요청을 80번 포트로 넘기고,
     
     https를 처리하기 위해 30101번 포트로 들어온 것을 443번 포트로 넘김.
     
     그리고 NGINX 인그레스 컨트롤러가 위치하는 네임스페이스를 ingress-nginx로 지정하고
     
     NGINX 인그레스 컨트롤러의 요구 사항에 따라 셀렉터를 ingress-nginx로 지정

9.  노트포트 서비스로 생성된 NGINX 인그레스 컨트롤러를 확인

   ```
   kubectl get service -n ingress-nginx
   ```

10. expose 명령으로 디플로이먼트들도 서비스로 노출.

   외부와 통신하기 위해 클러스터 내부에서만 사용하는 파드를 클러스터 외부에 노출할 수 있는 구역으로 옮기는 것.

   내부와 외부 네트워크를 분리해 관리하는 DMZ와 유사한 기능

   ```
   kubectl expose deployment in-hname-pod --name=hname-svc-default --port=80,443
   kubectl expose deployment in-ip-pod --name=ip-svc --port=80,443
   ```

11. 생성된 서비스를 점검해 디플로이먼트들이 서비스에 정상적으로 노출되는지 확인

    ````
    kubectl get service
    ````

12. 192.168.56.101:30100에 접속해 외부에서 접속되는 경로에 따라 다르게 작동하는지 확인

13. 192.168.56.101:30100/ip로 접속. 요청 방법과 파드의 ip가 반환되는지 확인

14. 192.168.56.101:30101로 접속해 HTTP 연결이 아닌 HTTPS 연결도 정상적으로 작동하는지 확인.

    30101은 HTTPS의 포트인 443번으로 변환해 접속함.

15. 192.168.56.101:30101/ip로 접속해 요청 방법과 파드의 IP 주소가 웹 브라우저에 표시되는지 확인



### 3) 클라우드에서 쉽게 구성 가능한 로드밸런서

```
앞에서 배운 연결 방식은 들어오는 요청을 모두 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고
이를 다시 쿠버네티스의 파드로 보내는 구조였음. 매우 비효율적.
쿠베네티스에서는 로드밸러서라는 서비스 타입을 제공해 간단한 구조로 파드를 외부에 노출하고 부하를 분산함.
로드밸런서를 사용하려면 로드밸런서를 이미 구현해 둔 서비스업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야 함.
클라우드에서 제공하는 쿠버네티스를 사용하고 있다면(EKS, GKE, AKS) 쿠버네티스 클러스터에 로드밸런서 서비스가 생성돼
외부와 통신할 수 있는 IP가 부여되고 외부와 통신할 수 있으며 부하도 분산됨.
```

```
kubectl expose deployment ex-lb --type=LoadBalancer --name=ex-svc
kubectl get service ex-svc
```



### 4) 온프레미스에서 로드밸런서를 제공하는 MetalLB

```
온프레미스에서 로드밸런서를 사용하려면 내부에 로드밸런서 서비스를 받아주는 구성이 필요함. MetalLB가 이를 지원.
MetalLB는 베어메탈(운영체제가 설치되지 않은 하드웨어)로 구성된 쿠버네티스에서도 로드밸런서를 사용할 수 있게 고안된 프로젝트.
MetalLb는 특별한 네트워크 설정이나 구성이 있는 것이 아니라 기존의 L2 네트워크(ARP/NDP)와 L3 네트워크(BGP)로 로드밸런서를 구현.
MetalLB 컨트롤러는 작동 방식(Protocol)을 정의하고 EXTERNAL-IP를 부여해 관리함.
MetalLB 스피커는 정해진 작동 방식(L2/ARP, L3/BGP)에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공.
이때 L2는 스피커 중에서 리더를 선출해 경로 제공을 총괄하게 함.
```

#### MetalLB로 온프레미스 쿠버네티스 환경에서 로드밸런서 서비스 구성

1. 디플로이먼트를 이용해 2종류의 파드를 생성. scale 명령으로 파드를 3개로 늘려 노드당 1개씩 파드 배포

   ```
   kubectl create deployment lb-hname-pods --image=sysnet4admin/echo-hname
   kubectl scale deployment lb-hname-pods --replicas=3
   kubectl create deployment lb-ip-pods --image=sysnet4admin/echo-ip
   kubectl scale deployment lb-ip-pods --replicas=3
   ```

2. 2종류의 파드가 3개씩 총 6개가 배포됐는지 확인

   ```
   kubectl get po
   ```

3. 인그레스와 마찬가지로 사전에 정의된 오브젝트 스펙으로 MetalLB를 구성.

   이렇게 하면 MetalLB에 필요한 요소가 모두 설치되고 독립적인 네임스페이스(metallb-system)도 함께 만들어짐.

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.4/metallb.yaml
   ```

4. 배포된 MetalLB의 파드가 5개(controller 1개, speaker 4개)인지 확인하고, IP와 상태도 확인

   ```
   kubectl get po -n metallb-system -o wide
   ```

5. 인그레스와 마찬가지로 MetalLB도 설정을 적용해야 함. 이때 오브젝트는 ConfigMap을 사용.

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.4/metallb-l2config.yaml
   ```

   - metallb-l2config.yaml 내용

     ```yaml
     apiVersion: v1
     kind: ConfigMap
     metadata:
       namespace: metallb-system
       name: config
     data:
       config: |
         address-pools:
         - name: nginx-ip-range
           # metallb에서 제공하는 로드밸런서의 동작 방식
           protocol: layer2
           # metallb에서 제공하는 로드밸런서의 Ext 주소
           addresses:
           - 192.168.56.11-192.168.56.13
     ```

6. ConfigMap이 생성됐는지 확인

   ```
   kubectl get configmap -n metallb-system
   ```

7. MetalLB의 설정이 올바르게 적용됐는지 확인

   ```
   kubectl get configmap -n metallb-system -o yaml
   ```

8. 각 디플로이먼트를 로드밸런서 서비스로 노출

   ```
   kubectl expose deployment lb-hname-pods --type=LoadBalancer --name=lb-hname-svc --port=80
   kubectl expose deployment lb-ip-pods --type=LoadBalancer --name=lb-ip-svc --port=80
   ```

9. 생성된 로드밸런서 서비스별로 CLUSTER-IP와 EXTERNAL-IP가 잘 적용됐는지 확인.

   특히 EXTERNAL-IP에 ConfigMap을 통해 부여한 IP를 확인.

   ```
   kubectl get service
   ```

10. EXTERNAL-IP가 잘 작동하는지 확인. 192.168.56.11로 접속

11. 192.168.56.12를 접속해 파드에 요청 방법과 IP가 표시되는지 확인

12. 셸 스크립트 실행. 로드밸런서 기능이 정상적으로 작동하면 192.168.56.11에서 반복적으로 결괏값 가져옴

    ```bash
    i=1; while true; do sleep 1; echo $((i++)) `curl --silent 192.168.56.11`; done
    ```

13. scale 명령으로 파드를 6개로 늘림

    ```
    kubectl scale deployment lb-hname-pods --replicas=6
    ```

14. 늘어난 파드 6개도 EXTERNAL-IP를 통해 접근되는지 확인



### 5) 부하에 따라 자동으로 파드 수를 조절하는 HPA

```
사용자가 갑자기 늘어날 때를 대비해 부하량에 따라 디플로이먼트의 파드 수를 유동적으로 관리하는 기능을 제공(HPA)
```

#### HPA 설정 방법

1. 디플로이먼트 생성

   ```
   kubectl create deployment hpa-hname-pods --image=sysnet4admin/echo-hname
   ```

2. 파드를 로드밸런서 서비스로 바로 설정

   ```
   kubectl expose deployment hpa-hname-pods --type=LoadBalancer --name=hpa-hname-svc --port=80
   ```

3. 설정된 로드밸러서 서비스와 부여된 IP 확인

   ```
   kubectl get service
   ```

4. HPA가 작동하려면 파드의 자원이 어느 정도 사용되는지 파악해야 함

   ```
   kubectl top po
   
   Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)
   --> 자원을 요청하는 설정이 없다며 에러가 생기고 진행되지 않음.
   ```

   HPA가 자원을 요청할 때 메트릭 서버를 통해 계측값을 전달받음.

   현재 메트릭 서버가 없기 때문에 에러가 발생. 계측값을 수집하고 전달해 주는 메트릭 서버를 설정해야 함.

5. 메트릭 서버 생성( 원본 : github.com/kubernetes-sigs/metrics-server )

   ```
   kubectl create -f ~/_Book_k8sInfra/ch3/3.3.5/metrics-server.yaml
   ```

6. 메트릭 서버 설정 후 결과 확인

   ```
   kubectl top po
   
   NAME                              CPU(cores)   MEMORY(bytes)   
   hpa-hname-pods-75f874d48c-rm5qt   0m           1Mi
   ```

   현재는 아무런 부하가 없으므로 CPU와 MEMORY 값이 매우 낮게 나옴

   scale 기준 값이 설정돼 있지 않아서 파드 증설 시점을 알 수 없음.

   파드에 부하가 걸리기 전에 scale이 실행되게 디플로이먼트에 기준 값을 기록함.

   이때 Deployment를 새로 배포하기보다는 기존에 배포한 디플로이먼트 내용을 edit 명령으로 직접 수정.

7. edit 명령으로 배포된 디플로이먼트 내용 확인

   ```
   kubectl edit deployment hpa-hname-pods
   
   40번째 줄 변경
   resources:
     requests:
       cpu: "10m"
     limits:
       cpu: "50m"
   
   m(milliunits) 1000m = 1CPU
   ```

   ```
   A copy of your changes has been stored to "/tmp/kubectl-edit-b7f26.yaml"
   error: Edit cancelled, no valid changes were saved.
   
   다음과 같은 에러 발생 시 기본 에디터 설정이 되어 있지 않아 들여쓰기 등 문법 적용이 안 될 가능성 있음.
   
   echo "export EDITOR=vim" >> ~/.bashrc
   source ~/.bashrc
   로 해결 했음.
   ```

8. 일정 시간이 지난 후 kubectl top po를 실행하면 스펙이 변경돼 새로운 파드가 생성된 것을 알 수 있음

9. hpa-hname-pods에 autoscale을 설정해서 특정 조건이 만족되는 경우에 자동으로 scale 명령이 수행되도록 함.

   min : 최소 파드 수, max : 최대 파드 수, cpu-percent는 CPU 사용량이 50%를 넘으면 autoscale.

   ```
   kubectl autoscale deployment hpa-hname-pods --min=1 --max=30 --cpu-percent=50
   ```

10. 테스트를 위해 터미널 2개를 활용해 watch kubectl top po와 watch kubectl get po 명령어 실행

11. HPA 테스트 위해 반복문 실행

    ```bash
    i=1; while true; do sleep 1; echo $((i++)) `curl --silent 192.168.56.11`; done
    ```

12. 부하량이 늘어남에 따라 파드가 새로 생성되는지 확인

13. 부하 분산으로 생성된 파드의 부하량이 증가하는지 확인

14. 부하를 생성하는 터미널 종료

15. 일정 시간이 지난 후 더 이상 부하가 없으면 autoscale의 최소 조건인 파드 1개의 상태로 돌아가기 위해

    파드가 종료되는 것을 확인

16. 사용하지 않는 파드는 모두 종료되고 1개만 남음
