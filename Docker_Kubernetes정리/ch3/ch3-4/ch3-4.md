# 3장 컨테이너를 다루는 표준 아키텍처, 쿠버네티스

## 4. 알아두면 쓸모 있는 쿠버네티스 오브젝트

```
디플로이먼트 외에도 용도에 따라 사용할 수 있는 다양한 오브젝트가 있음.
예를 들면, 데몬셋, 컨피그맵, PV, PVC, 스테이트풀셋 등이 있음.
```

### 1) 데몬셋

```
데몬셋은 디플로이먼트의 replicas의 노드 수만큼 정해져 있는 형태라고 할 수 있음(노드 하나당 파드 한 개만을 생성)
노드의 단일 접속 지점으로 노드 외부와 통실할 때 사용. 노드를 관리하는 파드라면 데몬셋으로 만드는게 가장 효율적.
```

#### 데몬셋

1. 현재 MetalLB의 스피커가 각 노드에 분포돼 있는 상태 확인

   ```
   kubectl get po -n metallb-system -o wide
   ```

2. 워커 노드를 1개 늘림. Vagrantfile 수정 후 vagrant up w4-k8s 실행

3. 오브젝트 상태 변화 감지

   ```
   kubectl get po -n metallb-system -o wide -w
   ```

4. 자동으로 추가된 노드에 설치된 스피커가 데몬셋이 맞는지 확인

   ```
   kubectl get po speaker-v9tsm -o yaml -n metallb-system
   ```



### 2) 컨피그맵

```
컨피그맵은 이름 그대로 설정을 목적으로 사용하는 오브젝트.
```

#### 컨피그맵으로 작성된 MetalLB의 IP 설정 변경

1. 테스트용 디플로이먼트를 생성

   ```
   kubectl create deployment cfgmap --image=sysnet4admin/echo-hname
   ```

2. cfgmap을 로드밸런서(MetalLB)를 통해 노출하고 이름 지정

   ```
   kubectl expose deployment cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
   ```

3. 생성된 서비스의 IP를 확인

   ```
   kubectl get service
   ```

4. 사전에 구성돼 있는 컨피그맵의 기존 IP를 변경

   ```bash
   sed -i 's/11/21/;s/13/23/' ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml
   ```

5. 컨피그맵 설정 파일에 apply를 실행해 변경된 설정 적용

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.2/metallb-l2config.yaml
   ```

6. Metallb와 관련된 모든 파드를 삭제함.

   삭제하고 나면 kubelet에서 해당 파드를 자동으로 모두 다시 생성함

   ```
   kubectl delete po --all -n metallb-system
   ```

7. 새로 생성된 MetalLB의 파드들을 확인

   ```
   kubectl get po -n metallb-system
   ```

8. 기존에 노출한 MetalLB 서비스를 삭제하고 동일한 이름으로 다시 생성해 새로운 컨피그맵을 적용한 서비스가 올라오게 함

   ```
   kubectl delete service cfgmap-svc
   kubectl expose deployment cfgmap --type=LoadBalancer --name=cfgmap-svc --port=80
   ```

9. 변경된 설정이 적용돼 새로운 MetalLB 서비스의 IP가 바뀌었는지 확인

   ```
   kubectl get service
   ```

10. 192.168.56.21로 접속해 파드의 이름이 화면에 표시되는지 확인



### 3) PV와 PVC

```
때때로 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해
공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있음.
쿠버네티스는 이런 경우를 위해 다양한 형태의 볼륨을 제공함

- 임시 : emptyDir
- 로컬 : host Path, local
- 원격 : persistentVolumeClaim, cephfs, cinder, csi, fc(fibre channel), flexVolume,
		flocker, glusterfs, iscsi, nfs, portworxVolume, quobyte, rbd, scaleIO, storageos, vsphereVolume
- 특수 목적 : downwardAPI, configMap, secret, azureFile, projected
- 클라우드 : awsElasticBlockStore, azureDisk, gcePersistentDisk

쿠버네티스는 필요할 때 PVC(PersistentVolumeClaim, 지속적으로 사용 가능한 볼륨 요청)를 요청해 사용.
PVC를 사용하려면 PV(PersistentVolume, 지속적으로 사용 가능한 볼륨)로 볼륨을 선언해야 함.
간단하게 PV는 볼륨을 사용할 수 있게 준비하는 단계이고, PVC는 준비된 볼륨에서 일정 공간을 할당받는 것.
```

#### NFS 볼륨 타입으로 PV와 PVC를 생성하고 파드에 마운트하기

1. PV로 선언할 볼륨을 만들기 위해 NFS 서버를 마스터 노드에 구성.

   공유되는 디렉터리는 /nfs_shared로 생성하고, 해당 디렉터리를 NFS로 받아들일 IP 영역은 192.168.56.0/24로 설정.

   옵션을 적용해 /etc/exports에 기록. 옵션에서 rw는 읽기/쓰기, sync는 쓰기 작업 동기화, no_root_squash는 root 계정 사용 의미.

   이때 nfs-utils.x86_64는 현재 CentOS에 이미 설치돼 있으므로 설치하지 않아도 됨.

   ```bash
   mkdir /nfs_shared && echo '/nfs_shared 192.168.56.0/24(rw,sync,no_root_squash)' >> /etc/exports
   ```

2. 해당 내용을 시스템에 적용해 NFS 서버를 활성화하고 다음에 시작할 때도 자동으로 적용되도록 함

   ```
   systemctl enable --now nfs
   ```

3. 오브젝트 스펙을 실행해 PV를 생성

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
   ```

   - 오브젝트 스펙

     ```yaml
     apiVersion: v1
     kind: PersistentVolume
     metadata:
       name: nfs-pv
     spec:
       # stroage는 실제로 사용하는 용량을 제한하는 것이 아니라 쓸 수 있는 양을 레이블로 붙이는 것과 같음.
       # 이는 현재 스토리지가 단순히 NFS로 설정되었기 때문
       capacity:
         storage: 100Mi
       # PV를 어떤 방식으로 사용할지를 정의한 부분.
       # ReadWriteMany는 여러 개의 노드가 읽고 쓸 수 있도록 마운트하는 옵션
       # ReadWriteOnce(하나의 노드에서만 볼륨을 읽고 쓸 수 있게 마운트)
       # ReadOnlyMany(여러 개의 노드가 읽도록 마운트)
       accessModes:
         - ReadWriteMany
       # persistentVolumeReclaimPolicy는 PV가 제거됐을 때 작동하는 방법을 정의하는 것
       # Retain을 사용하여 유지
       # Delete, Recycle, Deprecated 옵션이 있음
       persistentVolumeReclaimPolicy: Retain
       # NFS 서버의 연결 위치에 대한 설정
       nfs:
         server: 192.168.56.10
         path: /nfs_shared
     ```

4. 생성된 PV의 상태가 Available임을 확인

   ```
   kubectl get pv
   ```

5. 오브젝트 스펙을 실행해 PVC를 생성

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
   ```

   - 오브젝트 스펙

     ```yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: nfs-pvc
     spec:
       accessModes:
         - ReadWriteMany
       resources:
         requests:
           storage: 10Mi
     ```

     ```
     PVC는 PV와 구성이 거의 동일함.
     하지만 PV는 사용자가 요청할 볼륨 공간을 관리자가 만들고,
     PVC는 사용자(개발자)간 볼륨을 요청하는 데 사용한다는 점에서 차이가 있음.
     여기서 요청하는 storage: 10Mi는 동적 볼륨이 아닌 경우에는 레이블 정도의 의미를 가짐.
     ```

6. 생성된 PVC를 확인

   ```
   kubectl get pvc
   
   NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
   nfs-pvc   Bound    nfs-pv   100Mi      RWX                           3m27s
   
   상태 : Bound => PV와 PVC가 연결됨을 의미
   ```

7. PV 상태도 확인

   ```
   kubectl get pv
   ```

8. 생성한 PVC를 볼륨으로 사용하는 디플로이먼트 오브젝트 스펙을 배포

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc-deploy.yaml
   ```

   - nfs-pvc-deploy.yaml

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: nfs-pvc-deploy
     spec:
       replicas: 4
       selector:
         matchLabels:
           app: nfs-pvc-deploy
       template:
         metadata:
           labels:
             app: nfs-pvc-deploy
         spec:
           containers:
             # audit-trail 이미지를 가져옴.
             # 해당 컨테이너 이미지는 요청을 처리할 때 마다 접속 정보를 로그로 기록함
           - name: audit-trail
             image: sysnet4admin/audit-trail
             # 볼륨이 마운트될 위치로 /audit 지정
             volumeMounts:
             - name: nfs-vol
               mountPath: /audit
           # PVC로 생성된 볼륨을 마운트하기 위해서 nfs-pvc라는 이름을 사용
           volumes:
           - name: nfs-vol
             persistentVolumeClaim:
               claimName: nfs-pvc
     ```

9. 생성된 파드 확인

   ```
   kubectl get po
   ```

10. 생성한 파드 중 하나에 접속

    ```
    kubectl exec -it nfs-pvc-deploy-5fd9876c46-4gv5c -- /bin/bash
    ```

11. PVC 마운트 상태를 확인. 용량이 100Mi가 아닌 NFS 서버의 용량이 37G임을 확인.

    ```
    df -h
    
    Filesystem                   Size  Used Avail Use% Mounted on
    overlay                       37G  3.2G   34G   9% /
    tmpfs                        496M     0  496M   0% /dev
    tmpfs                        496M     0  496M   0% /sys/fs/cgroup
    192.168.56.10:/nfs_shared     37G  4.6G   33G  13% /audit
    /dev/mapper/centos_k8s-root   37G  3.2G   34G   9% /etc/hosts
    shm                           64M     0   64M   0% /dev/shm
    tmpfs                        496M   12K  496M   1% /run/secrets/kubernetes.io/serviceaccount
    tmpfs                        496M     0  496M   0% /proc/acpi
    tmpfs                        496M     0  496M   0% /proc/scsi
    tmpfs                        496M     0  496M   0% /sys/firmware
    ```

12. audit-trail 컨테이너의 기능을 테스트.

    외부에서 파드에 접속할 수 있도록 expose로 로드밸런서 서비스를 생성

    ```
    kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer --name=nfs-pvc-deploy-svc --port=80
    ```

13. 생성한 롤드밸런서 서비스의 IP 확인

    ```
    kubectl get service
    ```

14. 브라우저에서 IP로 접속해 파드 이름과 IP가 표시되는지 확인

15. 접속한 파드에서 ls /audit 명령을 실행해 접속 기록 파일이 남았는지 확인

    ```
    ls /audit
    audit_nfs-pvc-deploy-5fd9876c46-4gv5c.log
    
    cat /audit/audit_nfs-pvc-deploy-5fd9876c46-4gv5c.log 
    27/Aug/2022:21:25:44 +0900  172.16.132.21  GET
    ```

16. 마스터 노드에서 scale 명령으로 파드를 4개에서 8개로 증가시킴

    ```
    kubectl scale deployment nfs-pvc-deploy --replicas=8
    ```

17. 생성된 파드 확인

    ```
    kubectl get po
    ```

18. 최근에 증가한 파드 중 1개를 선택해 접속하고 기록된 로그가 동일한지 확인

    ```
    kubectl exec -it nfs-pvc-deploy-5fd9876c46-6vjpw -- /bin/bash
    cat /audit/audit_nfs-pvc-deploy-5fd9876c46-4gv5c.log
    ```

19. 다른 브라우저를 열고 192.168.56.21로 접속해 다른 파드 이름과 IP가 표시되는지 확인

20. 접속한 파드에서 ls /audit을 실행해 새로 추가된 audit 로그 확인

    ```
    ls /audit
    audit_nfs-pvc-deploy-5fd9876c46-4gv5c.log  audit_nfs-pvc-deploy-5fd9876c46-8lxtb.log
    
    cat /audit/audit_nfs-pvc-deploy-5fd9876c46-8lxtb.log 
    27/Aug/2022:21:31:02 +0900  172.16.103.156  GET
    ```

21. 기존에 접속한 파드에서도 동일한 로그가 기록돼 있는지 확인

    ```
    ls /audit
    audit_nfs-pvc-deploy-5fd9876c46-4gv5c.log  audit_nfs-pvc-deploy-5fd9876c46-8lxtb.log
    ```



#### NFS 볼륨을 파드에 직접 마운트하기

1. 사용자가 관리자와 동일한 단일 시스템이라면 PV와 PVC를 사용할 필요가 없음

   따라서 단순히 불륨을 마운트하는지 확인

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-ip.yaml
   ```

   - nfs-ip.yaml

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: nfs-ip
     spec:
       replicas: 4
       selector:
         matchLabels:
           app: nfs-ip
       template:
         metadata:
           labels:
             app: nfs-ip
         spec:
           containers:
           - name: audit-trail
             image: sysnet4admin/audit-trail
             volumeMounts:
             - name: nfs-vol
               mountPath: /audit
           # PV와 PVC를 거치지 않고 바로 NFS 서버로 접속
           volumes:
           - name: nfs-vol
             nfs:
               server: 192.168.56.10
               path: /nfs_shared
     ```

2. 새로 배포된 파드를 확인하고 그 중 하나에 접속

   ```
   kubectl get po
   kubectl exec -it nfs-ip-747cdc5465-c9d52 -- /bin/bash
   ```

3. 접속한 파드에서 ls /audit으로 동일한 NFS 볼륨을 바라보고 있음을 확인

   ```
   ls /audit
   audit_nfs-pvc-deploy-5fd9876c46-4gv5c.log  audit_nfs-pvc-deploy-5fd9876c46-8lxtb.log
   ```



#### 볼륨 용량을 제한하는 방법 1 (PVC로 PV에 요청되는 용량을 제한하기)

1. PVC로 PV를 요청할 때 용량을 제한하는 오브젝트 스펙 적용

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/limits-pvc.yaml
   ```

   - limits-pvc.yaml

     ```yaml
     apiVersion: v1
     kind: LimitRange
     metadata:
       name: storagelimits
     spec:
       limits:
       - type: PersistentVolumeClaim
       	# PVC를 통해 PV를 요청할 때 최소 1Mi에서 최대 5Mi로 용량을 제한
         max:
           storage: 5Mi
         min:
           storage: 1Mi
     ```

2. PV와 PVC를 새로 생성하고 PVC가 최대 용량 제한(5Mi)에 걸려 수행되지 못하는지 확인

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
   
   Error from server (Forbidden): error when creating "/root/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml": persistentvolumeclaims "nfs-pvc" is forbidden: maximum storage usage per PersistentVolumeClaim is 5Mi, but request is 10Mi
   ```

3. 용량 제한 설정을 삭제

   ```
   kubectl delete limitranges storagelimits
   ```



#### 볼륨 용량을 제한하는 방법 2 (스토리지 리소스에 대한 사용량 제한)

1. 총 누적 사용량을 제한하기 위해 다음 오브젝트 스펙 적용

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/quota-pvc.yaml
   ```

   - quota-pvc.yaml

     ```yaml
     apiVersion: v1
     kind: ResourceQuota
     metadata:
       name: storagequota
     spec:
       # PVC는 5개, 용량은 25Mi가 넘지 않도록 제한
       hard:
         persistentvolumeclaims: "5"
         requests.storage: "25Mi"
     ```

2. PV 3개 만든 후 PVC 3개를 요청해 25Mi 제한으로 더 이상 PVC가 수행되지 못하는지 확인

   ```
   kubectl apply -f nfs-pvc1.yaml 
   Error from server (Forbidden): error when creating "nfs-pvc1.yaml": persistentvolumeclaims "nfs-pvc1" is forbidden: exceeded quota: storagequota, requested: requests.storage=10Mi, used: requests.storage=20Mi, limited: requests.storage=25Mi
   ```

3. PVC를 생성하기 위해 설정한 리소스 제한을 삭제

   ```
   kubectl delete resourcequotas storagequota
   ```



### 4) 스테이트풀셋

```
지금까지는 파드가 replicas에 선언된 만큼 무작위로 생성될 뿐이었음.
파드가 만들어지는 이름과 순서를 예측해야 할 때가 있음.
주로 레디스, 주키퍼, 카산드라, 몽고DB 등의 마스터-슬레이브 구조 시스템에서 필요

스테이트풀셋을 사용하여 해결할 수 있음.
스테이트풀셋은 volumeClaimTemplates 기능을 사용해 PVC를 자동으로 생성할 수 있고,
각 파드가 순서대로 생성되기 때문에 고정된 이름, 불륨, 설정 등을 가질 수 있음.
그래서 StatefulSet(이전 상태를 기억하는 세트)라는 이름을 사용.
다만 효율성 면에서 좋은 구조가 아니므로 요구 사항에 맞게 적절히 사용하는 것이 좋음.
```

#### 스테이트풀셋 생성 과정

1. 스테이트풀셋 생성

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.4/nfs-pvc-sts.yaml
   ```

   - nfs-pvc-sts.yaml

     ```yaml
     apiVersion: apps/v1
     kind: StatefulSet
     metadata:
       name: nfs-pvc-sts
     spec:
       replicas: 4
       serviceName: sts-svc-domain #statefulset need it
       selector:
         matchLabels:
           app: nfs-pvc-sts
       template:
         metadata:
           labels:
             app: nfs-pvc-sts
         spec:
           containers:
           - name: audit-trail
             image: sysnet4admin/audit-trail
             volumeMounts:
             - name: nfs-vol # same name of volumes's name 
               mountPath: /audit
           volumes:
           - name: nfs-vol
             persistentVolumeClaim:
               claimName: nfs-pvc
     ```

2. 파드가 생성되는지 확인

   ```
   kubectl get po -w
   
   NAME            READY   STATUS              RESTARTS   AGE
   nfs-pvc-sts-0   0/1     ContainerCreating   0          4s
   nfs-pvc-sts-0   1/1     Running             0          16s
   nfs-pvc-sts-1   0/1     Pending             0          0s
   nfs-pvc-sts-1   0/1     Pending             0          0s
   nfs-pvc-sts-1   0/1     ContainerCreating   0          0s
   ```

3. expose 명령은 스테이트풀셋을 지원하지 않음.  (디플로이먼트, 파드, 레플리카셋, 레플리케이션 컨트롤러 지원)

   해결하려면 파일로 로드밸런서 서비스를 작성, 실행해야 함

   ```
   kubectl expose statefulset nfs-pvc-sts --type=LoadBalancer --name=nfs-pvc-sts-svc --port=80
   
   error: cannot expose a StatefulSet.apps
   ```

4. 스테이트풀셋을 노출하기 위한 서비스를 생성하고, 로드밸런서 서비스를 확인

   ```
   kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.4/nfs-pvc-sts-svc.yaml
   kubectl get service
   ```

   - nfs-pvc-sts-svc.yaml

     ```yaml
     apiVersion: v1
     kind: Service
     metadata:
       name: nfs-pvc-sts-svc
     spec:
       selector:
         app: nfs-pvc-sts
       ports:
         - port: 80
       type: LoadBalancer
     ```

5. 192.168.56.21에 접속해 파드 이름과 IP가 표시되는지 확인

6. 파드에 접속한 후 ls /audit -l로 새로 접속한 파드의 정보가 추가됐는지 확인

   ```
   kubectl exec -it nfs-pvc-sts-0 -- /bin/bash
   ls -l /audit
   ```

   ```
   일반적으로 스테이트풀셋은 volumeClaimTemplates를 이용해 자동으로 각 파드에 독립적인 스토리지를 할당해 구성할 수 있음.
   그러나 NFS 환경에서는 동적으로 할당받을 수 없음.
   ```

   





#### 일반적으로 스테이트풀셋은 헤드리스 서비스로 노출

```
헤드리스 서비스는 IP를 가지지 않는 서비스 타입으로 중요한 자원인 IP를 절약할 수 있을뿐만 아니라,
스테이트풀셋과 같은 상태를 가지고 있는 오브젝트를 모두 노출하지 않고 상태 값을 외부에 알리고 싶은 것만
선택적으로 노출하게 할 수 있음.
따라서 일반적으로는 스테이트풀셋은 헤드리스 서비스로 노출하나,
고정된 이름을 사용하면서 외부에 모든 스테이트풀셋을 노출하고자 하는 경우에는 노드포트나 로드밸런서 서비스로 노출할 수 있음.
```

##### 현재 구성에서 헤드리스 서비스로 노출하고자 하는 경우

```
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.4/sts-svc-domain.yaml
kubectl get service

NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes       ClusterIP   10.96.0.1    <none>        443/TCP   74m
sts-svc-domain   ClusterIP   None         <none>        80/TCP    24s
```

```
노출된 IP는 없지만 내부적으로 각 파드의 이름과 노출된 서비스 이름 등을 조합한 도메인 이름으로
쿠버네티스 클러스터 내에서 통신할 수 있는 상태가 됨(CoreDNS)
```

